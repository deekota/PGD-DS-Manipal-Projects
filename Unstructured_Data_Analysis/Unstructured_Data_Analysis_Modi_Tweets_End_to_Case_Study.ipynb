{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consumer key is the API key that a service provider (Twitter, Facebook, etc.) issues to a consumer (a service that wants to access a user's resources on the service provider). This key is what identifies the consumer.\n",
    "\n",
    "- Consumer secret is the consumer \"password\" that is used, along with the consumer key, to request access (i.e. authorization) to a user's resources from a service provider.\n",
    "\n",
    "- Access token is what is issued to the consumer by the service provider once the consumer completes authorization. This token defines the access privileges of the consumer over a particular user's resources. Each time the consumer wants to access the the user's data from that service provider, the consumer includes the access token in the API request to the service provider.\n",
    "\n",
    "- For further details, take a look at this useful slides from google:\n",
    "\n",
    "https://docs.google.com/presentation/d/1KqevSqe6ygWVj4U-wlarKU7-SVR79x-vjpR4gEc4A9Q/edit?pli=1#slide=id.g1697c74a_1_14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "api_key=\"iA0v8GU7uy4j9ueiNTt4kYrGf\"\n",
    "api_secret=\"IGeRFoenuF5YU93SBLuHN5sSmSMM4PswrwaL5y7RYOh5b5uml0\"\n",
    "access_token=\"2566469852-amCniFui7RavFfBcclbrB3nuXSqkcDvSscsCJf5\"\n",
    "access_secret=\"7WOC4nSV9WRVFgcjzRquEjG7KPSzQqkOhHzRZTPfywj6C\"\n",
    "\n",
    "auth=tweepy.OAuthHandler(api_key,api_secret)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    "api=tweepy.API(auth)\n",
    "\n",
    "# Most recent modi tweets - Scrapping from 5 pages:\n",
    "d={}\n",
    "ls=[]\n",
    "ls_hash=[]\n",
    "for i in range(6):\n",
    "    tweets=api.user_timeline('@narendramodi',page=i,count=200)\n",
    "    for tweet in tweets:\n",
    "        ls.append(tweet.text.strip())\n",
    "        try:\n",
    "            ls_hash.append(tweet.entities['hashtags'][0]['text'])\n",
    "        except:\n",
    "            ls_hash.append(None)\n",
    "d['Tweet_Text']=ls\n",
    "d['Tweet_Hash']=ls_hash\n",
    "df_tweet=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# UDF of Data Cleaning:\n",
    "def clean_text(st):\n",
    "    tt=TweetTokenizer()\n",
    "    stopwords_update=stopwords.words('english')\n",
    "    try:\n",
    "        clean_ls=[i for i in tt.tokenize(st.lower()) if i not in stopwords_update]\n",
    "        return(' '.join(clean_ls))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# UDF for Lemmatization:\n",
    "def lemma(st):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    try:\n",
    "        lem_st=[lemma.lemmatize(i) for i in st.split()]\n",
    "        return(' '.join(lem_st))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# UDF for Vectorization:\n",
    "def vector_Tfidf(df_col,grams_min,grams_max,max_fea):\n",
    "    tf=TfidfVectorizer(ngram_range=(grams_min,grams_max),max_features=max_fea)\n",
    "    tf_df=pd.DataFrame(tf.fit_transform(df_col).toarray(),columns=tf.get_feature_names())\n",
    "    return(tf_df)\n",
    "\n",
    "def vector_count(df_col,grams_min,grams_max,min_dff):\n",
    "    tf=CountVectorizer(ngram_range=(grams_min,grams_max),min_df=min_dff)\n",
    "    tf_df=pd.DataFrame(tf.fit_transform(df_col).toarray(),columns=tf.get_feature_names())\n",
    "    return(tf_df)\n",
    "\n",
    "# UDF for Sentiment score:\n",
    "def sentiment(df_col):\n",
    "    ls_sent=[]\n",
    "    for i in range(len(df_col)):\n",
    "        analyzer=SentimentIntensityAnalyzer()\n",
    "        ls_sent.append(analyzer.polarity_scores(df_col.iloc[i])['compound'])\n",
    "    return(ls_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_clean_text=[]\n",
    "for i in range(len(df_tweet)):\n",
    "    ls_clean_text.append((clean_text(df_tweet.iloc[i].values[0])))\n",
    "    \n",
    "df_tweet_clean=pd.DataFrame(ls_clean_text,columns=['Clean_Review_Text'])\n",
    "df_tweet_clean.dropna(inplace=True)\n",
    "for i in range(len(df_tweet_clean)):\n",
    "    try:\n",
    "        df_tweet_clean.iloc[i]+' '+df_tweet['Tweet_Hash'].iloc[i]\n",
    "    except:\n",
    "        df_tweet_clean.iloc[i]+' '+str(df_tweet['Tweet_Hash'].iloc[i])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_df is used for removing data values that appear too frequently, also known as \"corpus-specific stop words\".\n",
    "\n",
    "- For example:\n",
    "\n",
    "    - max_df = 0.50 means \"It ignores terms that appear in more than 50% of the documents\".\n",
    "\n",
    "    - max_df = 25 means \"It ignores terms that appear in more than 25 documents\".\n",
    "\n",
    "- The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus the default setting does not ignore any terms.\n",
    "\n",
    "- min_df is used for removing terms that appear too infrequently.\n",
    "\n",
    "- For example:\n",
    "\n",
    "- min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "\n",
    "- min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "\n",
    "- The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DTM using CountVectorizer; Set min_df = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to below URL before using CountVecotrizer\n",
    "- https://stackoverflow.com/questions/57424183/how-to-force-sklearn-countvectorizer-to-not-remove-special-characters-i-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#diwali</th>\n",
       "      <th>#hunarhaat</th>\n",
       "      <th>#janjankabudget</th>\n",
       "      <th>#mannkibaat</th>\n",
       "      <th>#republicday</th>\n",
       "      <th>@bjp4india</th>\n",
       "      <th>@flotus</th>\n",
       "      <th>@gotabayar</th>\n",
       "      <th>@jairbolsonaro</th>\n",
       "      <th>@melaniatrump</th>\n",
       "      <th>...</th>\n",
       "      <th>victory</th>\n",
       "      <th>vision</th>\n",
       "      <th>welcome</th>\n",
       "      <th>welfare</th>\n",
       "      <th>wishes</th>\n",
       "      <th>wishing</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>worked</th>\n",
       "      <th>working</th>\n",
       "      <th>youngsters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   #diwali  #hunarhaat  #janjankabudget  #mannkibaat  #republicday  \\\n",
       "0        0           0                0            0             0   \n",
       "1        0           0                0            0             0   \n",
       "2        0           0                0            0             0   \n",
       "3        0           0                0            0             0   \n",
       "4        0           0                0            0             0   \n",
       "\n",
       "   @bjp4india  @flotus  @gotabayar  @jairbolsonaro  @melaniatrump  ...  \\\n",
       "0           0        0           0               0              0  ...   \n",
       "1           0        0           0               0              0  ...   \n",
       "2           0        0           0               0              0  ...   \n",
       "3           0        0           0               0              0  ...   \n",
       "4           0        0           0               0              0  ...   \n",
       "\n",
       "   victory  vision  welcome  welfare  wishes  wishing  wonderful  worked  \\\n",
       "0        0       0        0        0       0        0          0       0   \n",
       "1        0       0        0        0       0        0          0       0   \n",
       "2        0       0        0        0       0        0          0       0   \n",
       "3        0       0        0        0       0        0          0       0   \n",
       "4        0       0        0        0       0        0          0       0   \n",
       "\n",
       "   working  youngsters  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the columns contains special charaters like hash tags, Count Vecotrizer ignores them and only provide text excluding the\n",
    "#specail charaters as the name of the features. We need to use the specaial parameters not to ignore the special charaters.\n",
    "#The default regexp select tokens of 2 or more alphanumeric characters \n",
    "# punctuation is completely ignored and always treated as a token separator.\n",
    "\n",
    "def vector_count(df_col,grams_min,grams_max,min_dff):\n",
    "    tf=CountVectorizer(ngram_range=(grams_min,grams_max),min_df=min_dff,token_pattern='[a-zA-Z0-9!#$*=?@]+')\n",
    "    tf_df=pd.DataFrame(tf.fit_transform(df_col).toarray(),columns=tf.get_feature_names())\n",
    "    return(tf_df)\n",
    "df_tweet_clean_DTM=vector_count(df_tweet_clean['Clean_Review_Text'],1,1,5)\n",
    "col_update=[]\n",
    "for i in df_tweet_clean_DTM.columns:\n",
    "    if len(i)>5:\n",
    "        col_update.append(i)\n",
    "df_tweet_clean_DTM=df_tweet_clean_DTM[col_update]\n",
    "df_tweet_clean_DTM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using KMeans algorithm, cluster the tweets in to 4 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array(df_tweet_clean_DTM)\n",
    "from sklearn.cluster import KMeans\n",
    "km=KMeans(n_clusters=4,random_state=0)\n",
    "y=km.fit_predict(x)\n",
    "df_tweet_clean_DTM['Clusters']=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 words in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 top 5 terms: ['elections' 'taking' 'president' 'victory' 'congratulations']\n",
      "Cluster 2 top 5 terms: ['wonderful' 'congratulate' 'statehood' 'government' 'people']\n",
      "Cluster 3 top 5 terms: ['delighted' 'tomorrow' 'addressing' 'greetings' 'towards']\n",
      "Cluster 4 top 5 terms: ['discussions' 'attended' 'extensive' 'excellent' 'meeting']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotad\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\kotad\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\kotad\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\kotad\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "clust_cen=km.cluster_centers_\n",
    "top_5=np.argsort(clust_cen)[:,-5:]\n",
    "ls_c1=[]\n",
    "ls_c2=[]\n",
    "ls_c3=[]\n",
    "ls_c4=[]\n",
    "\n",
    "for i in range(len(top_5)):\n",
    "    if i ==0:\n",
    "        ls_c1.append(top_5[i])\n",
    "    elif i ==1:\n",
    "        ls_c2.append(top_5[i])\n",
    "    elif i ==2:\n",
    "        ls_c3.append(top_5[i])\n",
    "    elif i ==3:\n",
    "        ls_c4.append(top_5[i])\n",
    "        \n",
    "col=np.array(df_tweet_clean_DTM.columns)\n",
    "print('Cluster 1 top 5 terms:',col[ls_c1])\n",
    "print('Cluster 2 top 5 terms:',col[ls_c2])\n",
    "print('Cluster 3 top 5 terms:',col[ls_c3])\n",
    "print('Cluster 4 top 5 terms:',col[ls_c4])             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 hashtags in each group ( only applicable for twitter data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusetr 0 top5 #tags in ascending order of their count: ['#republicday' '#hunarhaat' '#diwali' '#janjankabudget' '#mannkibaat']\n",
      "Clusetr 1 top5 #tags in ascending order of their count: ['#janjankabudget' '#republicday' '#diwali' '#mannkibaat' '#hunarhaat']\n",
      "Clusetr 2 top5 #tags in ascending order of their count: ['#diwali' '#republicday' '#janjankabudget' '#hunarhaat' '#mannkibaat']\n",
      "Clusetr 3 top5 #tags in ascending order of their count: ['#mannkibaat' '#republicday' '#hunarhaat' '#diwali' '#janjankabudget']\n"
     ]
    }
   ],
   "source": [
    "clust_cen=km.cluster_centers_\n",
    "top_5=np.argsort(clust_cen)\n",
    "col=df_tweet_clean_DTM.columns\n",
    "d={}\n",
    "for i in range(len(top_5)):\n",
    "    ls=[]\n",
    "    for j in top_5[i]:\n",
    "        if col[j].startswith('#'):\n",
    "            ls.append(j)\n",
    "    d[i]=ls\n",
    "col_np=np.array(df_tweet_clean_DTM.columns)\n",
    "for i in d.keys():\n",
    "    print('Clusetr',i,'top5 #tags in ascending order of their count:',col_np[d[i]])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
